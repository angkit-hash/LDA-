{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ee582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all necessary modules \n",
    "import numpy as np         \n",
    "import tqdm\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "import spacy\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "737bb73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA():    #Class for LDA\n",
    "    def calculate_coh(corpus, dictionary, k, a, b,lemm):#Function for optimal number of topics based on best coherence scores \n",
    "\n",
    "        my_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                               id2word=dictionary,\n",
    "                                               num_topics=k, \n",
    "                                               random_state=10,\n",
    "                                               chunksize=100,\n",
    "                                               passes=8,\n",
    "                                               alpha=a,\n",
    "                                               eta=b)\n",
    "       \n",
    "        end_model = CoherenceModel(model=my_model, texts= lemm, dictionary=dictionary, coherence='c_v')\n",
    "\n",
    "        return end_model.get_coherence()\n",
    "    \n",
    "    def give_best(corp,ids,my_texts): #For finding number of topics and the best parameters for the models based on coherence score\n",
    "        minimum = 5            #minimum topics              \n",
    "        maximum = 11             #maximum topics \n",
    "        jump = 1\n",
    "        wide = range(minimum, maximum, jump)\n",
    "        \n",
    "        safe = {}\n",
    "        safe['Required'] = {}\n",
    "          \n",
    "        B = list(np.arange(0.01, 1, 0.5))#Range for values of beta\n",
    "        B.append('symmetric')\n",
    "        \n",
    "        A = list(np.arange(0.01, 1, 0.5)) #Range for values of alpha\n",
    "        A.append('symmetric')\n",
    "        A.append('asymmetric')\n",
    "  \n",
    "        count = len(corp)\n",
    "        corp_se = [gensim.utils.ClippedCorpus(corp, int(count*0.70)), \n",
    "                       corp]\n",
    "\n",
    "        index = ['70% of Corpus', '100% of Corpus']\n",
    "\n",
    "        solutions = {   'My_VAS': [],\n",
    "                         'my_T': [],\n",
    "                         'my_A': [],\n",
    "                         'my_B': [],\n",
    "                         'my_COHVAL': []\n",
    "                        }\n",
    "       \n",
    "        if 2 == 2:\n",
    "            pbar = tqdm.tqdm(total=(len(A)*len(B)*len(index)*len(wide)))\n",
    "            for j in range(len(corp_se)):   # for different configuration  \n",
    "                for z in wide: #range of topics                 \n",
    "                    for x in A: #range of alpha\n",
    "                        for y in B: #range ofbeta\n",
    "                            ob = LDA.calculate_coh(corpus=corp_se[j], dictionary=ids, \n",
    "                                                          k=z, a=x, b=y, lemm = my_texts)                        \n",
    "                            solutions['My_VAS'].append(index[j])\n",
    "                            solutions['my_T'].append(z)\n",
    "                            solutions['my_A'].append(x)\n",
    "                            solutions['my_B'].append(y)\n",
    "                            solutions['my_COHVAL'].append(ob)\n",
    "                            pbar.update(1)\n",
    "            final = LDA.get_optimum(pd.DataFrame(solutions))\n",
    "            pbar.close()\n",
    "        return final\n",
    "     \n",
    "    def get_optimum(frame):# To get the optimal values of number of topics,alpha,beta based on best coherence score\n",
    "        return frame.loc[frame['my_COHVAL'].idxmax()]\n",
    "        \n",
    "    def fit(corpus, dictionary, k, a, b): # for model fit\n",
    "        my_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                               id2word=dictionary,\n",
    "                                               num_topics=k, \n",
    "                                               random_state=10,\n",
    "                                               chunksize=100,\n",
    "                                               passes=8,\n",
    "                                               alpha=a,\n",
    "                                               eta=b)\n",
    "        return my_model\n",
    "    \n",
    "    def topics(corpus, dictionary, k, a, b): #predict new instances based on best LDA model with optimal number of topics based on best coherence score\n",
    "        my_model = LDA.fit(corpus, dictionary, k, a, b)\n",
    "        return my_model\n",
    "    \n",
    "    def display(model):\n",
    "        for my_id, my_top in model.print_topics(-1):\n",
    "            print(\"Topic: {} \\nWords: {}\".format(my_id, my_top ))\n",
    "            print(\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5491491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing():   #Class for preprocessing    \n",
    "    \n",
    "    def shut(done,wait):\n",
    "        return [[var_1 for var_1 in simple_preprocess(str(cod)) if var_1 not in wait] for cod in done]\n",
    "    \n",
    "    def modification(pros,bi_gram, allowings =[ 'ADJ','NOUN', 'VERB']):\n",
    "        gone = []\n",
    "        for var in bi_gram:\n",
    "            cod = pros(\" \".join(var)) \n",
    "            gone.append([o.lemma_ for o in cod if o.pos_ in allowings])\n",
    "        return gone\n",
    "        \n",
    "    def call(dec): # For necessary preprocessing\n",
    "        pages = pd.read_csv('C:/Users/fighter/Desktop/papers.csv')\n",
    "       \n",
    "        pages = pages.drop(columns=['year','pdf_name' ,'event_type', \n",
    "                                       'abstract','title' ,'id' ], axis=1)\n",
    "        if (dec == 1):\n",
    "            pages = pages.sample(1)\n",
    "        else:\n",
    "            pages = pages.sample(80) #Sameples 80 units\n",
    "        pages['paper_text_processed'] = pages['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "        pages['paper_text_processed'] = pages['paper_text_processed'].map(lambda x: x.lower())\n",
    "        my_data = pages.paper_text_processed.values.tolist()\n",
    "        done = list(preprocessing.sending(my_data))\n",
    "        bis = gensim.models.Phrases(done, min_count=8, threshold=96) # higher threshold fewer phrases.\n",
    "        bii = gensim.models.phrases.Phraser(bis)\n",
    "        wait = stopwords.words('english')         \n",
    "        no = preprocessing.shut(done,wait)\n",
    "        bi_gram = preprocessing.bi_grams(no,bii)\n",
    "        pros = spacy.load(\"en_core_web_sm\", disable=[ 'ner','parser'])\n",
    "        llm = preprocessing.modification(pros,bi_gram, allowings =[ 'ADJ','NOUN', 'VERB'])\n",
    "        ids = corpora.Dictionary(llm) #making the dictionary\n",
    "        my = llm\n",
    "        corp = [ids.doc2bow(var) for var in my]\n",
    "\n",
    "        return ids,my,corp\n",
    "    \n",
    "    def sending(my_data):    \n",
    "        for var in my_data:\n",
    "            yield(gensim.utils.simple_preprocess(str(var), deacc=True)) \n",
    "        \n",
    "    def bi_grams(no,bii):        # for bigram  \n",
    "        return [bii[var] for var in no]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6625908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/144 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.024*\"neuron\" + 0.012*\"cell\" + 0.011*\"network\" + 0.010*\"synaptic\" + 0.008*\"pyloric\" + 0.007*\"input\" + 0.007*\"pattern\" + 0.006*\"transmitter\" + 0.005*\"property\" + 0.005*\"potential\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.013*\"algorithm\" + 0.012*\"use\" + 0.008*\"model\" + 0.008*\"factor\" + 0.008*\"theory\" + 0.007*\"learn\" + 0.007*\"graph\" + 0.006*\"result\" + 0.006*\"player\" + 0.005*\"game\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.010*\"use\" + 0.009*\"model\" + 0.009*\"problem\" + 0.008*\"parameter\" + 0.008*\"network\" + 0.008*\"function\" + 0.008*\"set\" + 0.007*\"time\" + 0.006*\"process\" + 0.006*\"result\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.011*\"algorithm\" + 0.009*\"set\" + 0.008*\"instance\" + 0.008*\"use\" + 0.007*\"function\" + 0.007*\"label\" + 0.006*\"model\" + 0.006*\"datum\" + 0.006*\"graph\" + 0.005*\"method\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.010*\"use\" + 0.010*\"model\" + 0.009*\"set\" + 0.009*\"datum\" + 0.009*\"density\" + 0.007*\"target\" + 0.007*\"feature\" + 0.006*\"saliency\" + 0.006*\"number\" + 0.006*\"give\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.015*\"image\" + 0.012*\"use\" + 0.011*\"network\" + 0.011*\"model\" + 0.010*\"object\" + 0.008*\"learn\" + 0.006*\"set\" + 0.006*\"input\" + 0.006*\"training\" + 0.005*\"algorithm\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.011*\"tree\" + 0.011*\"split\" + 0.010*\"filter\" + 0.009*\"datum\" + 0.008*\"use\" + 0.008*\"point\" + 0.008*\"figure\" + 0.008*\"node\" + 0.007*\"distribution\" + 0.007*\"training\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.011*\"use\" + 0.009*\"model\" + 0.007*\"learn\" + 0.006*\"variable\" + 0.006*\"feature\" + 0.006*\"function\" + 0.006*\"set\" + 0.006*\"network\" + 0.006*\"training\" + 0.006*\"show\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.022*\"model\" + 0.011*\"use\" + 0.007*\"network\" + 0.006*\"set\" + 0.006*\"layer\" + 0.006*\"learn\" + 0.006*\"distribution\" + 0.006*\"state\" + 0.005*\"result\" + 0.005*\"output\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.017*\"model\" + 0.012*\"algorithm\" + 0.011*\"use\" + 0.008*\"function\" + 0.008*\"feature\" + 0.007*\"set\" + 0.006*\"variable\" + 0.006*\"learn\" + 0.005*\"distribution\" + 0.005*\"time\"\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 3/216 [17:55<21:12:43, 358.51s/it]\n",
      "100%|██████████| 144/144 [1:44:44<00:00, 43.64s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             My_VAS  my_T        my_A       my_B  my_COHVAL\n",
      "0     70% of Corpus     5        0.01       0.01   0.279563\n",
      "1     70% of Corpus     5        0.01       0.51   0.268059\n",
      "2     70% of Corpus     5        0.01  symmetric   0.279872\n",
      "3     70% of Corpus     5        0.51       0.01   0.279563\n",
      "4     70% of Corpus     5        0.51       0.51   0.271173\n",
      "..              ...   ...         ...        ...        ...\n",
      "139  100% of Corpus    10   symmetric       0.51   0.328332\n",
      "140  100% of Corpus    10   symmetric  symmetric   0.328706\n",
      "141  100% of Corpus    10  asymmetric       0.01   0.324620\n",
      "142  100% of Corpus    10  asymmetric       0.51   0.329542\n",
      "143  100% of Corpus    10  asymmetric  symmetric   0.331527\n",
      "\n",
      "[144 rows x 5 columns]\n",
      "Optimal number of topics are :  7\n",
      "Topic: 0 \n",
      "Words: 0.003*\"matrix\" + 0.003*\"algorithm\" + 0.003*\"singular\" + 0.003*\"value\" + 0.003*\"decomposition\" + 0.003*\"input\" + 0.003*\"output\" + 0.003*\"figure\" + 0.003*\"equation\" + 0.003*\"learn\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.003*\"singular\" + 0.003*\"matrix\" + 0.003*\"algorithm\" + 0.003*\"output\" + 0.003*\"value\" + 0.003*\"figure\" + 0.003*\"input\" + 0.003*\"decomposition\" + 0.003*\"learn\" + 0.003*\"orthogonal\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.003*\"singular\" + 0.003*\"matrix\" + 0.003*\"algorithm\" + 0.003*\"value\" + 0.003*\"figure\" + 0.003*\"decomposition\" + 0.003*\"input\" + 0.003*\"output\" + 0.003*\"equation\" + 0.003*\"encoder\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.003*\"matrix\" + 0.003*\"algorithm\" + 0.003*\"singular\" + 0.003*\"output\" + 0.003*\"value\" + 0.003*\"figure\" + 0.003*\"learn\" + 0.003*\"equation\" + 0.003*\"encoder\" + 0.003*\"orthogonal\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.003*\"singular\" + 0.003*\"matrix\" + 0.003*\"algorithm\" + 0.003*\"value\" + 0.003*\"output\" + 0.003*\"input\" + 0.003*\"decomposition\" + 0.003*\"learn\" + 0.003*\"figure\" + 0.003*\"orthogonal\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.003*\"matrix\" + 0.003*\"algorithm\" + 0.003*\"singular\" + 0.003*\"value\" + 0.003*\"output\" + 0.003*\"figure\" + 0.003*\"input\" + 0.003*\"encoder\" + 0.003*\"decomposition\" + 0.003*\"vector\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.026*\"matrix\" + 0.024*\"algorithm\" + 0.023*\"singular\" + 0.014*\"value\" + 0.014*\"output\" + 0.013*\"input\" + 0.013*\"figure\" + 0.012*\"decomposition\" + 0.011*\"equation\" + 0.011*\"learn\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():   #Main function     \n",
    "    ids,my_texts,corp = preprocessing.call(0)  \n",
    "    untrained = LDA.fit(corp, ids, 10 , 0.1, 0.1) # Fit model which is \"NOT OPTIMAL\"\n",
    "    LDA.display(untrained)\n",
    "    values = LDA.give_best(corp,ids,my_texts)  #evaluate optimal number of topics based on best coherence scores\n",
    "    print(\"Optimal number of topics are : \", values[1])\n",
    "    ids,my_texts,corp = preprocessing.call(1) # predict new instances(sampled 1 unit) based on best LDA model with optimal number of topics based on best coherence score\n",
    "    model = LDA.topics(corp, ids, values[1], values[2], values[3])      \n",
    "    LDA.display(model)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e93084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc270a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3e159b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145db81b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff75e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
